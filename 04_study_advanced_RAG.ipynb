{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12fc8290",
   "metadata": {},
   "source": [
    "# RAG 検索の精度向上策(advanced RAG)について\n",
    "\n",
    "- 『LangChain と LangGraph による RAG・AI エージェント[実践]入門』より\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf86566",
   "metadata": {},
   "source": [
    "## ・シンプルな RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8671ce2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8916a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import GitLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53ac32de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n"
     ]
    }
   ],
   "source": [
    "### ベクトル化対象データ取得\n",
    "def file_filter(file_path: str) -> bool:\n",
    "    return file_path.endswith(\".mdx\")\n",
    "\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./langchain\",\n",
    "    branch=\"master\",\n",
    "    file_filter=file_filter,\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d1678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ドキュメントのベクトル化\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "# db = Chroma.from_documents(documents, embeddings, persist_directory=\"./chroma_db\") # ベクトルストアの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ec9ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 永続化したChromaDBへの接続\n",
    "# 後続のセッションや別プロセスから読み込むとき\n",
    "db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
    "# docs = db.similarity_search(\"検索クエリ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e5f1a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。LangChainは、アプリケーションのライフサイクルの各段階を簡素化することを目的としています。具体的には、以下のような機能を提供しています。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。また、LangGraphを利用して、状態を持つエージェントを構築し、ストリーミングや人間の介入をサポートします。\\n\\n2. **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\\n\\nLangChainは、チャットモデルや埋め込みモデル、ベクトルストアなど、さまざまな技術と統合されており、開発者が異なるプロバイダー間で簡単に切り替えられるように標準化されたインターフェースを提供します。また、複雑なアプリケーションの構築を支援するためのオーケストレーション機能も備えています。'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### シンプルRAG実装\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    '''\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈：\"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "質問：{question}\n",
    "'''\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": retriever,\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"LangChainの概要を教えて。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f473fcc0",
   "metadata": {},
   "source": [
    "#### インデクシングの工夫について\n",
    "\n",
    "- インデクシングについては以下の工夫がある\n",
    "  - 適切な大きさでチャンク化\n",
    "  - ドキュメントのカテゴリなどをメタデータ化 etc...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6f7289",
   "metadata": {},
   "source": [
    "## 1. 検索クエリの工夫\n",
    "\n",
    "### 1-1. HyDE (Hypothetical Document Embeddings)\n",
    "\n",
    "- ユーザーの質問に対して LLM に仮説的な回答を推論させ、その出力を埋め込みベクトルの類似度検索に使用する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7298542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。LangChainは、開発、運用、デプロイの各段階を簡素化することを目的としています。具体的には、以下のような特徴があります。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。また、LangGraphを利用して、状態を持つエージェントを構築することができます。\\n\\n2. **運用**: LangSmithを使用してアプリケーションを監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\\n\\nLangChainは、さまざまなプロバイダーと統合し、標準インターフェースを提供することで、開発者が異なるコンポーネントを簡単に切り替えたり、組み合わせたりできるようにしています。また、複雑なアプリケーションのオーケストレーションをサポートするためのLangGraphや、アプリケーションの可視化と評価を行うLangSmithといったツールも提供しています。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 仮説的な回答を生成するChainの実装\n",
    "hypothtical_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "次の質問に回答する一文を書いてください。\n",
    "\n",
    "質問：{question}\n",
    "\"\"\"\n",
    ")\n",
    "# 仮説的な回答を生成するChain\n",
    "hypothetical_chain = hypothtical_prompt | model | StrOutputParser()\n",
    "\n",
    "### RAGのChain実装\n",
    "hyde_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": hypothetical_chain\n",
    "        | retriever,  # 仮説的な回答を生成するChainの出力をretrieverに渡す\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "hyde_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48838de6",
   "metadata": {},
   "source": [
    "- ユーザーの質問よりも仮説的な回答のほうが埋め込みベクトルの類似度検索に適しているという想定の手法。そのため、LLM が仮説的な回答を推論しやすいケースに有効。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b844e6",
   "metadata": {},
   "source": [
    "### 1-2. 複数の検索クエリの生成\n",
    "\n",
    "- 複数の検索クエリを使うことで、適切なドキュメントが検索結果に含まれやすくなる可能性がある。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eed6303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLMアプリケーションのライフサイクルの各段階を簡素化します。具体的には、以下のような機能を提供しています。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。LangGraphを利用することで、状態を持つエージェントを構築し、ストリーミングや人間の介入をサポートします。\\n\\n2. **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\\n\\nLangChainは、LLMや関連技術（埋め込みモデルやベクトルストアなど）に対する標準インターフェースを実装しており、数百のプロバイダーと統合されています。また、複数のオープンソースライブラリで構成されており、開発者が簡単にさまざまなコンポーネントを組み合わせて使用できるように設計されています。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### LangChainの「with_structured_output」を使い、検索クエリのリストを生成する\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class QueryGenerationOutput(BaseModel):\n",
    "    queries: list[str] = Field(..., description=\"検索クエリのリスト\")\n",
    "\n",
    "\n",
    "query_generation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "質問に対してベクターデータベースから関連文書を検索するために、３つの異なる検索クエリを生成してください。\n",
    "距離ベースの類似度検索の限界を克服するために、ユーザーの質問に対して複数の視点を提供することが目標です。\n",
    "\n",
    "質問：{question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "query_generation_chain = (\n",
    "    query_generation_prompt\n",
    "    | model.with_structured_output(QueryGenerationOutput)\n",
    "    | (lambda x: x.queries)\n",
    ")\n",
    "\n",
    "### chain実装\n",
    "multi_query_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": query_generation_chain | retriever.map(),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "multi_query_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81a6f75",
   "metadata": {},
   "source": [
    "- `retriever.map()`では、通常 retriever が str を受け取って list[Document]を返すのに対して、list[str]を受け取って、list[list[Document]]を返すように変換している。\n",
    "- `map`は LangChain の Runnable が提供するメソッドの一つで、もとの Runnable に対して引数と戻り値を list 化するメソッド\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9afd0cb",
   "metadata": {},
   "source": [
    "## 2. 検索後の工夫\n",
    "\n",
    "- https://smith.langchain.com/o/5cb6609d-ce31-51cc-962f-6052d0aff4cc/\n",
    "- https://app.tavily.com/home\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ef5fc",
   "metadata": {},
   "source": [
    "### 2-1. RAG-Fusion\n",
    "\n",
    "- 複数の検索クエリを生成し、それらの検索結果を RRF で並べる RAG 手法\n",
    "- 複数の検索結果の順位を融合して並べるアルゴリズムは RRF(Reciprocal Rang Fusion)を利用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f159b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(\n",
    "    retriever_outputs: list[list[Document]],\n",
    "    k: int = 60,\n",
    ") -> list[str]:\n",
    "    # 各ドキュメントのコンテンツ（文字列）とそのスコアの対応を保持する辞書を準備\n",
    "    content_score_mapping = {}\n",
    "\n",
    "    # 検索クエリごとにループ\n",
    "    for docs in retriever_outputs:\n",
    "        # 検索結果のドキュメントごとにループ\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # ドキュメントのコンテンツを取得\n",
    "            content = doc.page_content\n",
    "\n",
    "            # 初めて登場したコンテンツの場合はスコアを０で初期化\n",
    "            if content not in content_score_mapping:\n",
    "                content_score_mapping[content] = 0\n",
    "\n",
    "            # (1/(順位 + k))のスコアを加算\n",
    "            content_score_mapping[content] += 1 / (rank + k)\n",
    "\n",
    "    # スコアの大きい順にソート\n",
    "    ranked = sorted(content_score_mapping.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return [content for content, _ in ranked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288fedf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。LangChainは、開発、運用、デプロイの各段階を簡素化することを目的としています。\\n\\n### 主な特徴\\n1. **標準化されたコンポーネントインターフェース**: LangChainは、さまざまなAIアプリケーションに必要なコンポーネントの標準インターフェースを提供し、異なるプロバイダー間での切り替えを容易にします。\\n   \\n2. **オーケストレーション**: 複数のコンポーネントやモデルを組み合わせて複雑なアプリケーションを構築するためのオーケストレーション機能を提供します。これにより、制御フローや状態管理が可能になります。\\n\\n3. **可観測性と評価**: LangChainは、アプリケーションの動作を監視し、迅速に評価するためのツールを提供します。これにより、開発者はアプリケーションのパフォーマンスを把握しやすくなります。\\n\\n### エコシステム\\nLangChainは、以下のような複数のオープンソースライブラリで構成されています。\\n- **langchain-core**: チャットモデルやその他のコンポーネントの基本抽象。\\n- **langgraph**: LangChainコンポーネントを組み合わせて生産準備が整ったアプリケーションを構築するためのオーケストレーションフレームワーク。\\n- **LangSmith**: LLMアプリケーションのトレース、監視、評価を行うためのプラットフォーム。\\n\\n### 使い方\\nLangChainを使用することで、開発者は簡単にアプリケーションを構築し、最適化し、デプロイすることができます。具体的には、チャットボットやエージェント、情報検索システムなど、さまざまなアプリケーションを作成することが可能です。\\n\\nLangChainは、開発者がAIアプリケーションを迅速に構築し、運用するための強力なツールを提供します。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_fusion_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": query_generation_chain | retriever.map() | reciprocal_rank_fusion,\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_fusion_chain.invoke(\"LangChainの概要を教えて。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a43f9",
   "metadata": {},
   "source": [
    "### 2-2. リランクモデル\n",
    "\n",
    "RRF では、複数の検索結果の順位を融合して並べた。別観点として、1 つの検索結果の順位についても、改めて並べ替えること（リランク）が有用な場合がある。  \n",
    "検索結果を並べ替える方法の一つがリランクモデル（リランク用の機械学習モデル）を使うこと。\n",
    "リランクモデルは、埋め込みベクトルの類似度検索よりも計算コストが高い代わりに、ランキングの精度が高いモデルを使用する。\n",
    "<br>\n",
    "ここでは、Cohere のリランクモデルを利用する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cca3315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "367c8b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLMアプリケーションのライフサイクルの各段階を簡素化します。具体的には、以下のような機能があります。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。LangGraphを利用することで、状態を持つエージェントを作成し、ストリーミングや人間の介入をサポートします。\\n\\n2. **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\\n\\nLangChainは、チャットモデルや埋め込みモデル、ベクトルストアなどの関連技術に対する標準インターフェースを実装しており、数百のプロバイダーと統合されています。また、複数のオープンソースライブラリで構成されており、開発者は必要なコンポーネントを選択して使用することができます。'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rerank(inp: dict[str, Any], top_n: int = 3) -> list[Document]:\n",
    "    \"\"\"\n",
    "    リランクモデルを使って、検索結果を再評価し、上位のドキュメントを返す。\n",
    "\n",
    "    Args:\n",
    "        inp (dict[str, Any]): 入力データ。'documents'キーにリスト形式のDocumentが含まれる。\n",
    "        top_n (int): 上位のドキュメント数。\n",
    "\n",
    "    Returns:\n",
    "        list[Document]: リランクされたドキュメントのリスト。\n",
    "    \"\"\"\n",
    "    question = inp[\"question\"]\n",
    "    documents = inp[\"documents\"]\n",
    "\n",
    "    cohere_reranker = CohereRerank(model=\"rerank-multilingual-v3.0\", top_n=top_n)\n",
    "    return cohere_reranker.compress_documents(documents=documents, query=question)\n",
    "\n",
    "\n",
    "rerank_rag_chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"documents\": retriever}\n",
    "    | RunnablePassthrough.assign(context=rerank)\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rerank_rag_chain.invoke(\"LangChainの概要を教えて。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c562a207",
   "metadata": {},
   "source": [
    "## 3. 複数の Retriever を使く工夫\n",
    "\n",
    "### 3-1. LLM によるルーティング\n",
    "\n",
    "- 質問内容により、検索対象の Retriever の使い分け\n",
    "- ここでは、LangChain の公式ドキュメントの検索と Web 検索を質問内容により使い分ける RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baec991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd8f337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。LangChainは、アプリケーションのライフサイクルの各段階を簡素化することを目的としており、以下のような機能を提供しています。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。また、LangGraphを利用して、状態を持つエージェントを構築し、ストリーミングや人間の介入をサポートします。\\n\\n2. **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\\n\\nLangChainは、チャットモデルや埋め込みモデル、ベクトルストアなど、さまざまな技術と統合されており、開発者が異なるプロバイダー間で簡単に切り替えられるように標準化されたインターフェースを提供します。また、複雑なアプリケーションのオーケストレーションをサポートするために、LangGraphというライブラリも提供されています。\\n\\n全体として、LangChainは、AIアプリケーションの開発を容易にし、開発者が迅速に高品質なアプリケーションを構築できるようにすることを目指しています。'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 各検索器用意\n",
    "# ドキュメント検索用\n",
    "# LangSmithのトレースがわかりやすくなるように、with_configメソッドでrun_nameを設定\n",
    "langchain_document_retriever = retriever.with_config(\n",
    "    {\"Run_name\": \"LangChain Document Retriever\"}\n",
    ")\n",
    "\n",
    "# web検索用\n",
    "web_retriever = TavilySearchAPIRetriever(k=3).with_config({\"Run_name\": \"Web Retriever\"})\n",
    "\n",
    "\n",
    "### ユーザーの入力を元にLLMがRetrieverを選択するChain\n",
    "class Route(str, Enum):\n",
    "    langchain_document = \"langchain_document\"\n",
    "    web = \"web\"\n",
    "\n",
    "\n",
    "class RouteOutput(BaseModel):\n",
    "    route: Route\n",
    "\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "質問に回答するために適切なRetrieverを選択してください。\n",
    "\n",
    "質問：{question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "route_chain = (\n",
    "    route_prompt | model.with_structured_output(RouteOutput) | (lambda x: x.route)\n",
    ")\n",
    "\n",
    "\n",
    "### ルーティングの結果を踏まえて検索するroute_retriever関数と、処理全体の流れのChain(route_rag_chain)を実装\n",
    "def route_retriever(inp: dict[str, Any]) -> list[Document]:\n",
    "    \"\"\"\n",
    "    ユーザーの質問に基づいて適切なRetrieverを選択し、検索結果を返す。\n",
    "\n",
    "    Args:\n",
    "        inp (dict[str, Any]): 入力データ。'question'キーにユーザーの質問が含まれる。\n",
    "\n",
    "    Returns:\n",
    "        list[Document]: 選択されたRetrieverからの検索結果。\n",
    "    \"\"\"\n",
    "    question = inp[\"question\"]\n",
    "    route = inp[\"route\"]\n",
    "\n",
    "    if route == Route.langchain_document:\n",
    "        return langchain_document_retriever.invoke(\n",
    "            question\n",
    "        )  # 辞書ではなく、文字列を渡す！！！\n",
    "    elif route == Route.web:\n",
    "        return web_retriever.invoke(question)  # 辞書ではなく、文字列を渡す！！！\n",
    "\n",
    "    raise ValueError(f\"Unknown retriever: {retriever}\")\n",
    "\n",
    "\n",
    "route_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"route\": route_chain,\n",
    "    }\n",
    "    | RunnablePassthrough.assign(context=route_retriever)\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "route_rag_chain.invoke(\"LangChainの概要を教えて。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfbad3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'東京の今日の天気は曇のち雨です。昼頃から所々で雨雲が湧き、夜は広く雨になる予報です。'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route_rag_chain.invoke(\"東京の今日の天気は？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d23964",
   "metadata": {},
   "source": [
    "### 3-2. ハイブリッド検索\n",
    "\n",
    "- 複数の Retriever の検索結果を組み合わせて利用\n",
    "- 例として、TF-IDF や BM25 によるベクトル検索を組み合わせたハイブリッド検索を構築（埋め込みベクトルの類似度検索と BM25 を使った検索の組み合わせ）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcf1de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57707503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLMアプリケーションのライフサイクルの各段階を簡素化することを目的としています。具体的には、以下のような機能を提供しています。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティ統合を使用してアプリケーションを構築できます。また、LangGraphを利用して、状態を持つエージェントを構築し、ストリーミングや人間の介入をサポートします。\\n\\n2. **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\\n\\nLangChainは、さまざまなモデルや関連コンポーネントに対して標準化されたインターフェースを提供し、開発者がプロバイダー間で簡単に切り替えたり、コンポーネントを組み合わせたりできるようにします。また、複雑なアプリケーションのオーケストレーションをサポートし、アプリケーションの可観測性や評価を向上させるためのツールも提供しています。'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 各retrieverを用意\n",
    "# 埋め込みベクトル\n",
    "chroma_retriever = retriever.with_config({\"Run_name\": \"Chroma_Retriever\"})\n",
    "\n",
    "# BM25\n",
    "# documentsは、最初の方でGitLoaderで取得したドキュメントを使用\n",
    "bm25_retriever = BM25Retriever.from_documents(documents).with_config(\n",
    "    {\"Run_name\": \"BM25_Retriever\"}\n",
    ")\n",
    "\n",
    "\n",
    "### ハイブリッド検索のChainを実装\n",
    "# ここでは、chroma_retrieverとbm25_retrieverの検索結果の順位をRRFで融合して並べている\n",
    "hybrid_retriever = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"chroma_documents\": chroma_retriever,\n",
    "            \"bm25_documents\": bm25_retriever,\n",
    "        }\n",
    "    )\n",
    "    | (\n",
    "        lambda x: [x[\"chroma_documents\"], x[\"bm25_documents\"]]\n",
    "    )  # 並列実行の結果をリストにまとめる\n",
    "    | reciprocal_rank_fusion  # RRFで順位を融合\n",
    ")\n",
    "\n",
    "hyde_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": hybrid_retriever,\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "hyde_rag_chain.invoke(\"LangChainの概要を教えて。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538db11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
