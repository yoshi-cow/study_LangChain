{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9abbaac",
   "metadata": {},
   "source": [
    "# LCEL(LangChain Expression Language)復習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf993e7",
   "metadata": {},
   "source": [
    "## 1. 基本LCEL\n",
    "* 各Runnableを「|」でつなぐ\n",
    "* Runnableの実行方法\n",
    "    * invoke\n",
    "    * stream\n",
    "    * batch - 複数入力をまとめて処理\n",
    "* 以下は、非同期実行用\n",
    "    * ainvoke\n",
    "    * astream\n",
    "    * abatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6198d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cf44db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カレーのレシピをご紹介します。シンプルで美味しい基本のカレーを作りましょう。\n",
      "\n",
      "### 材料（4人分）\n",
      "- 鶏肉（もも肉または胸肉）: 400g\n",
      "- 玉ねぎ: 2個\n",
      "- にんじん: 1本\n",
      "- じゃがいも: 2個\n",
      "- カレールー: 1箱（約200g）\n",
      "- サラダ油: 大さじ2\n",
      "- 水: 800ml\n",
      "- 塩: 適量\n",
      "- 胡椒: 適量\n",
      "- お好みでガーリックパウダーや生姜: 適量\n",
      "\n",
      "### 作り方\n",
      "1. **材料の下ごしらえ**:\n",
      "   - 鶏肉は一口大に切り、塩と胡椒をふっておきます。\n",
      "   - 玉ねぎは薄切り、にんじんは輪切り、じゃがいもは一口大に切ります。\n",
      "\n",
      "2. **炒める**:\n",
      "   - 大きめの鍋にサラダ油を熱し、玉ねぎを中火で炒めます。玉ねぎが透明になるまで炒めます。\n",
      "   - 鶏肉を加え、表面が白くなるまで炒めます。\n",
      "\n",
      "3. **野菜を加える**:\n",
      "   - にんじんとじゃがいもを鍋に加え、全体をよく混ぜます。\n",
      "\n",
      "4. **煮る**:\n",
      "   - 水を加え、強火で煮立たせます。煮立ったら、アクを取り除き、蓋をして中火にし、約15分煮ます。\n",
      "\n",
      "5. **カレールーを加える**:\n",
      "   - 火を止めてカレールーを加え、よく溶かします。再び弱火にし、10分ほど煮込みます。お好みでガーリックパウダーや生姜を加えて風味を調整します。\n",
      "\n",
      "6. **味を調える**:\n",
      "   - 最後に塩で味を調整し、全体がなじむまでさらに5分ほど煮ます。\n",
      "\n",
      "7. **盛り付け**:\n",
      "   - ご飯と一緒に盛り付けて、お好みで福神漬けやらっきょうを添えて完成です。\n",
      "\n",
      "### おすすめのトッピング\n",
      "- 煮卵\n",
      "- チーズ\n",
      "- 青ねぎやパセリの刻んだもの\n",
      "\n",
      "この基本のカレーはアレンジがしやすいので、野菜や肉を変えて自分好みのカレーを楽しんでください！"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# chain\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "for chunk in chain.stream({\"dish\": \"カレー\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d768cf",
   "metadata": {},
   "source": [
    "## 2. Zero-shot CoTでステップバイステップで考えさせるChainと結論を作成するChainの連結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29aeb57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b7a32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'結論: \\\\(10 + 2 * 3 = 16\\\\) です。'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Zero-shot CoTでステップバイステップで考えさせるChain\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "cot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザーの質問にステップバイステップで回答してください。\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cot_chain = cot_prompt | model | output_parser\n",
    "\n",
    "\n",
    "### 上記回答から結論を抽出するChain\n",
    "summarize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ステップバイステップで考えた回答から結論だけ抽出してください。\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "summarize_chain = summarize_prompt | model | output_parser\n",
    "\n",
    "\n",
    "### ２つのchainを連結\n",
    "cot_summarize_chain = cot_chain | summarize_chain\n",
    "cot_summarize_chain.invoke({\"question\": \"10+2*3\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa4aef6",
   "metadata": {},
   "source": [
    "## 3. RunnableLambda - 任意の関数のRunnable化\n",
    "* LLMの生成したテキストに対して、小文字を大文字に変換する処理をChain化\n",
    "\n",
    "### 3-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f5412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a39bf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO! HOW CAN I ASSIST YOU TODAY?\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{input}\",)\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 小文字を大文字に変換する関数\n",
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "chain = prompt | model | output_parser | RunnableLambda(upper)\n",
    "\n",
    "output = chain.invoke({\"input\": \"Hello!\"})\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5afb610",
   "metadata": {},
   "source": [
    "### 3-2. chainデコレータ(@chain)を使った実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddfe8f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO! HOW CAN I ASSIST YOU TODAY?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "chain = prompt | model | output_parser | upper\n",
    "\n",
    "output = chain.invoke({\"input\": \"Hello!\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f64e5",
   "metadata": {},
   "source": [
    "### 3-3. RunnableLambdaへの自動変換\n",
    "* 明示的にRunnableLambdaを作成しなくても、Runnableと任意の関数を「|」で接続できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0a3d967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO! HOW CAN I ASSIST YOU TODAY?\n"
     ]
    }
   ],
   "source": [
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "chain = prompt | model | output_parser | upper\n",
    "\n",
    "output = chain.invoke({\"input\": \"Hello!\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d3b39",
   "metadata": {},
   "source": [
    "#### Runnableの入力型と出力型に注意すること\n",
    "```python\n",
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "chain = prompt | model | upper\n",
    "\n",
    "output = chain.invoke({\"input\": \"Hello!\"})\n",
    "```\n",
    "\n",
    "上記コードでは、以下のエラーが発生する。\n",
    "```bash\n",
    "AttributeError: 'AIMessage' object has no attribute 'upper'\n",
    "```\n",
    "\n",
    "原因として、modelがAIMessageを出力するのに対して、自作upper関数は入力としてstrを期待しているため。\n",
    "そのため、前のRunnerの出力と次のRunnerの入力の型は一致していないといけない。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54acf8d8",
   "metadata": {},
   "source": [
    "### 自作関数のストリーミング処理について\n",
    "* ジェネレータ関数を用いることで、自作関数をストリーミング対応にできる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d426d140",
   "metadata": {},
   "source": [
    "## 4. RunnableParallelによる複数Runnerの並列化\n",
    "\n",
    "### 4-1. 一つのinputに対して2つのLLMの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73647665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import pprint\n",
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5117f6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimistic_opinion': '生成AIの進化は本当に素晴らしいですね！技術が進むことで、私たちの生活がより便利で豊かになる可能性が広がっています。クリエイティブな作業や問題解決の手助けをしてくれるAIが増えてきて、私たちのアイデアを実現するためのパートナーとして活躍しています。これからも新しい発見や革新が続くことで、私たちの未来はますます明るくなるでしょう！どんな素晴らしいことが待っているのか、ワクワクしますね！',\n",
      " 'pessimistic_opinion': '生成AIの進化は確かに目覚ましいものですが、その裏には多くの懸念が潜んでいます。技術が進化することで、私たちの仕事が奪われたり、情報の信頼性が低下したりするリスクが高まっています。さらに、AIが生成するコンテンツが人間の創造性を脅かし、私たちの思考や感情に悪影響を及ぼす可能性もあります。結局のところ、便利さの裏には常に危険が潜んでいるのです。私たちがこの技術をどれだけ賢く使おうとも、その影響を完全にコントロールすることは難しいでしょう。'}\n"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 楽観的な意見を言うLLM\n",
    "optimistic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは楽観主義者です。ユーザーの入力に対して楽観的な意見を下さい。\"),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "optimistic_chain = optimistic_prompt | model | output_parser\n",
    "\n",
    "# 楽観的な意見を言うLLM\n",
    "pessimistic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは悲観主義者です。ユーザーの入力に対して悲観的な意見を下さい。\"),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "pessimistic_chain = pessimistic_prompt | model | output_parser\n",
    "\n",
    "# 並列実行\n",
    "parallel_chain = RunnableParallel(\n",
    "    {\n",
    "        \"optimistic_opinion\": optimistic_chain,\n",
    "        \"pessimistic_opinion\": pessimistic_chain,\n",
    "    }\n",
    ")\n",
    "\n",
    "output = parallel_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fdcc2a",
   "metadata": {},
   "source": [
    "### 4-2. RunnableParallelの出力をRunnableの入力に連結する\n",
    "* 楽観的な意見と悲観的な意見を出したうえで、客観的にまとめるChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "504a934e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成AIの進化については、楽観的な意見と悲観的な意見が存在します。楽観的な見方では、生成AIの技術が進むことで私たちの生活が便利で豊かになり、クリエイティブな作業や問題解決のパートナーとしての役割を果たすことが期待されています。この進化により、新しい発見や革新が続き、未来が明るくなる可能性があるとされています。\n",
      "\n",
      "一方で、悲観的な見方では、生成AIの進化には多くの懸念が伴います。仕事の喪失や情報の信頼性の低下、さらにはオリジナリティやクリエイティビティの喪失といったリスクが指摘されています。便利さの裏には問題が潜んでおり、この技術をどのように扱うかが重要であるとされています。\n",
      "\n",
      "総じて、生成AIの進化は多くの可能性を秘めている一方で、慎重な対応が求められる複雑な状況であると言えるでしょう。\n"
     ]
    }
   ],
   "source": [
    "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは客観的なAIです。２つの意見をまとめてください。\"),\n",
    "        (\"human\", \"楽観的意見：{optimistic_opinion}\\n悲観的意見：{pessimistic_opinion}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "synthesize_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"optimistic_opinion\": optimistic_chain,\n",
    "            \"pessimistic_opinion\": pessimistic_chain,\n",
    "        }\n",
    "    )\n",
    "    | synthesize_prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "output = synthesize_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6956c1",
   "metadata": {},
   "source": [
    "### 4-3. RunnableParallelへの自動変換\n",
    "* キーがstrで値がRunnable（または、Runnableに自動変換できる関数など）であるdictは、RunnableParallelに自動変換される。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize_chain = (\n",
    "    {\n",
    "        \"optimistic_opinion\": optimistic_chain,\n",
    "        \"pessimistic_opinion\": pessimistic_chain,\n",
    "    } # 上記は、RunnableParallelに自動変換される\n",
    "    | synthesize_prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069f67dd",
   "metadata": {},
   "source": [
    "### 4-4. itemgetterを使う\n",
    "* `itemgetter`を使うと、dictなどから値を取り出す関数を簡単に作れる.\n",
    "以下は、『{\"topic\": \"生成AIの進化について\"}』というdictから、itemgetter(\"topic\")を使ってtopicを取り出す例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67c1c227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成AIの進化について\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "topic_getter = itemgetter(\"topic\")\n",
    "topic = topic_getter({\"topic\": \"生成AIの進化について\"})\n",
    "print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38287b",
   "metadata": {},
   "source": [
    "以下では、『{\"topic\": \"生成AIの進化について\"}』から`itemgetter(\"topic\")`で値を取り出し、`ChatPromptTemplate`の`{topic}`の箇所に穴埋めしている例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46965d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**楽観的意見**：生成AIの進化は、私たちの生活をより便利で豊かにする可能性を秘めています。技術の進歩により、クリエイティブな作業や問題解決の支援を行うAIが登場し、私たちのアイデアや想像力を引き出す手助けをしてくれるでしょう。未来において、生成AIが社会に与える影響や新しい発見、革新に対する期待が高まります。\n",
      "\n",
      "**悲観的意見**：生成AIの進化には多くの懸念が伴います。技術の進化が進むことで、仕事の喪失や情報の信頼性の低下といったリスクが増大します。また、AIが生成するコンテンツが人間の創造性を脅かし、思考や感情に悪影響を及ぼす可能性もあります。便利さの裏には常に危険が潜んでいるため、慎重な対応が求められます。\n"
     ]
    }
   ],
   "source": [
    "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは客観的なAIです。{topic}について２つの意見をまとめてください。\"),\n",
    "        (\"human\", \"楽観的意見：{optimistic_opinion}\\n悲観的意見：{pessimistic_opinion}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "synthesize_chain = (\n",
    "    {\"optimistic_opinion\": optimistic_chain,\n",
    "     \"pessimistic_opinion\": pessimistic_chain,\n",
    "     \"topic\": itemgetter(\"topic\") # RunnableLambda(itemgetter(\"topic\"))に自動変換される\n",
    "     }\n",
    "     | synthesize_prompt\n",
    "     | model\n",
    "     | output_parser\n",
    ")\n",
    "\n",
    "output = synthesize_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff67812",
   "metadata": {},
   "source": [
    "## 5. RunnablePassthrough - 入力をそのまま出力する\n",
    "* RunnableParallelで、その要素の一部で入力の値をそのまま出力したい場合などに利用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9078984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58e0e20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "東京の今日の天気は曇のち雨です。昼頃から所々で雨雲が湧き、夜は広く雨になる予報です。\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    以下の文脈だけを踏まえて質問に答えてください。\n",
    "                                          \n",
    "    文脈：'''\n",
    "    {context}\n",
    "    '''\n",
    "\n",
    "    質問：{question}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# webデータベクトル検索器\n",
    "retriever = TavilySearchAPIRetriever(k=3)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "output = chain.invoke(\"東京の今日の天気は？\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66973a",
   "metadata": {},
   "source": [
    "* RunnablePassthroughは、入力値をそのまま出力する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183097ee",
   "metadata": {},
   "source": [
    "### 5-2. assign - RunnableParallelの出力に値を追加\n",
    "* 上記では、LLMが生成した最終的な回答だけがChain全体の出力になっている。しかし、retreiverの検索結果もChain全体の出力に含めたい時がある。その時に使えるメソッド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "125c307d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '東京の今日の天気は「曇のち雨」です。昼頃から所々で雨雲が湧き、夜には広く雨が降る予報です。',\n",
      " 'context': [Document(metadata={'title': '東京都の天気 - 日本気象協会 tenki.jp', 'source': 'https://tenki.jp/forecast/3/16/', 'score': 0.7803451, 'images': []}, page_content='# tenki.jp 雨雲レーダー) 天気図 PM2.5分布予測 地震情報 日直予報士 熱中症情報 東京都の天気 ### 05月29日(木) 東京都の天気 曇のち雨 曇のち雨 曇のち雨 曇のち雨 曇のち雨 曇のち雨 曇のち雨 曇のち雨 晴時々曇 曇のち雨 曇のち雨 曇のち雨 曇のち雨 曇のち雨 最新の天気履歴 渋谷区 曇のち雨 豊島区 曇のち雨 江東区 曇のち雨 港区 曇のち雨 大田区 曇のち雨 府中市 曇のち雨 奥多摩町 曇のち雨 関東・甲信 ### 気象予報士のポイント解説(日直予報士) newマーク 関東甲信\\u3000今日29日は天気下り坂\\u3000昼頃から所々で雨雲が湧く\\u3000夜は広く雨に 関東甲信\\u3000今日29日は天気下り坂\\u3000昼頃から所々で雨雲が湧く\\u3000夜は広く雨に newマーク 今日29日は気圧が低下\\u3000沖縄や九州から近畿は影響度「大」の所も\\u3000頭痛など注意 今日29日は気圧が低下\\u3000沖縄や九州から近畿は影響度「大」の所も\\u3000頭痛など注意 今日29日\\u3000九州から関東は次第に雨\\u3000東北と北海道は晴れて気温上昇\\u3000真夏日も 今日29日\\u3000九州から関東は次第に雨\\u3000東北と北海道は晴れて気温上昇\\u3000真夏日も ### 東京都各地の天気 #### 東京23区 #### 多摩 #### 伊豆諸島北部(大島) #### 伊豆諸島南部(八丈島) #### 小笠原諸島(父島) ### おすすめ記事 LINEの友達追加 ### 天気ガイド 雨雲 ### 注目の情報 アプリに便利なサブスクプラン開始 「tenki.jpライト」なら現在地の雨雲接近通知が受け取れる！ 新サービス「気圧予報」 気圧変化を確認して、頭痛やめまい、倦怠感といった症状に備えましょう。 X（旧Twitter） tenki.jpの公式X（旧Twitter） 最新の気象・防災情報や、生活に役立つ情報を毎日リアルタイムに配信中！ 天気予報 観測 防災情報 天気図 指数情報 レジャー天気 季節特集 天気ニュース X(旧：Twitter) Youtube Facebook Instagram LINEの友達追加 tenki.jp tenki.jp tenki.jp 登山天気 tenki.jp 登山天気 全国のコンテンツ tenki.jpトップ 天気予報 観測 防災情報 天気図'),\n",
      "             Document(metadata={'title': '東京（東京）の天気 - Yahoo!天気・災害', 'source': 'https://weather.yahoo.co.jp/weather/jp/13/4410.html', 'score': 0.71185225, 'images': []}, page_content='東京（東京）の天気 - Yahoo! - Yahoo! Yahoo! | 6月25日(水)  曇一時雨曇一時雨   - *29*℃[-1] - *25*℃[-1]  |  |  |  |  |  | | --- | --- | --- | --- | --- | | 時間 | 0-6 | 6-12 | 12-18 | 18-24 | | 降水 | --- | --- | --- | 50％ |   風：  南東の風後北東の風  波：  0.5メートル | 6月26日(木)  曇一時雨曇一時雨   - *31*℃[+2] - *25*℃[0]  |  |  |  |  |  | | --- | --- | --- | --- | --- | | 時間 | 0-6 | 6-12 | 12-18 | 18-24 | | 降水 | 70％ | 20％ | 20％ | 40％ |   風：  北の風後南の風２３区西部では南西の風やや強く  波：  0.5メートル後1.5メートル | - 6月25日(水) - 6月26日(木) (C) LY Corporation Yahoo! (C) LY Corporation Yahoo! cpt_n=safetymail&cpt_m=bnr&cpt_s=yahoo&cpt_c=weatherbnr) Yahoo!'),\n",
      "             Document(metadata={'title': '東京の天気予報 - ウェザーニュース', 'source': 'https://weathernews.jp/onebox/tenki/tokyo/', 'score': 0.6778372, 'images': []}, page_content='{{item.cityname}} {{item.high}} {{item.low}} è\\x87ªç\\x84¶ã\\x83»å\\xad£ç¯\\x80ã\\x83»ã\\x83¬ã\\x82¸ã\\x83£ã\\x83¼æ\\x83\\x85å ± æ¢\\x85é\\x9b¨æ\\x83\\x85å ± è\\x8a±ç²\\x89é£\\x9bæ\\x95£æ\\x83\\x85å ± å\\x9c°é\\x9c\\x87æ\\x83\\x85å ± å\\x8f°é¢¨æ\\x83\\x85å ± é\\x95·æ\\x9c\\x9fäº\\x88å ± å\\x8f°é¢¨æ\\x83\\x85å ± å\\x9c°é\\x9c\\x87æ\\x83\\x85å ± æ´¥æ³¢æ\\x83\\x85å ± ç\\x81«å±±æ\\x83\\x85å ± æ¸\\x9bç\\x81½æ\\x83\\x85å ± é\\x81¿é\\x9b£æ\\x83\\x85å ± ã\\x81\\x95ã\\x81\\x8fã\\x82\\x89é\\x96\\x8bè\\x8a±æ\\x83\\x85å ± ã\\x82¹ã\\x82\\xadã\\x83¼ï¼\\x86ã\\x82¹ã\\x83\\x8eã\\x83\\x9cæ\\x83\\x85å ± è\\x8a±ç\\x81«å¤©æ°\\x97 Ch. ã\\x81»ã\\x81\\x9fã\\x82\\x8bæ\\x83\\x85å ± ã\\x81\\x82ã\\x81\\x98ã\\x81\\x95ã\\x81\\x84æ\\x83\\x85å ± æ\\x98\\x9fç©ºæ\\x83\\x85å ± å\\x88\\x9dæ\\x97¥ã\\x81®å\\x87ºæ\\x83\\x85å ± å\\x88\\x9dè©£æ\\x83\\x85å ± ç´\\x85è\\x91\\x89æ\\x83\\x85å ± ã\\x82¤ã\\x83«ã\\x83\\x9fã\\x83\\x8dã\\x83¼ã\\x82·ã\\x83§ã\\x83³æ\\x83\\x85å ± è\\x8a±ç²\\x89é£\\x9bæ\\x95£æ\\x83\\x85å ± å¤©æ°\\x97ç\\x97\\x9bäº\\x88å ± ç\\x86±ä¸\\xadç\\x97\\x87äº\\x88å ± ç´«å¤\\x96ç·\\x9aäº\\x88å ± æ\\x9c\\x8dè£\\x85äº\\x88å ± ã\\x81\\x8aæ´\\x97æ¿¯æ\\x83\\x85å ± Labs Ch. # æ\\x9d±äº¬ã\\x81®å¤©æ°\\x97äº\\x88å ± ## {{pref\\\\_jp}}{{cityname}}ã\\x81®1æ\\x99\\x82é\\x96\\x93æ¯\\x8eã\\x81®å¤©æ°\\x97 {{daily.date}}æ\\x97¥{{daily.youbi\\\\_str}} {{hourly.time}} {{hourly.prec}} {{hourly.temp}} {{hourly.wndspd}} ## {{pref\\\\_jp}}{{cityname}}ã\\x81®é\\x80±é\\x96\\x93å¤©æ°\\x97äº\\x88å ±ï¼\\x8810æ\\x97¥é\\x96\\x93ï¼\\x89 {{item.date}}   {{item.youbi\\\\_str}} {{item.high}} {{item.low}} {{item.pop}} ## å¸\\x82å\\x8cºç\\x94ºæ\\x9d\\x91å\\x88¥ã\\x81®å¤©æ°\\x97äº\\x88å ± {{point.rank}} ({{point.pref}}) ({{point.dateRecord}}) ## å\\x90\\x84ã\\x82¨ã\\x83ªã\\x82¢ã\\x81®å¤©æ°\\x97äº\\x88å ± {{point.rank}} ({{point.pref}}) ({{point.dateRecord}}) ### è\\x87ªç\\x84¶ã\\x83»å\\xad£ç¯\\x80ã\\x83»ã\\x83¬ã\\x82¸ã\\x83£ã\\x83¼æ\\x83\\x85å ±')],\n",
      " 'question': '東京の今日の天気は？'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": retriever,\n",
    "} | RunnablePassthrough.assign(answer=prompt | model | StrOutputParser())\n",
    "\n",
    "output = chain.invoke(\"東京の今日の天気は？\")\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279ae95",
   "metadata": {},
   "source": [
    "または、以下の方法がある："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3f2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": retriever,\n",
    "    }\n",
    ").assign(answer=prompt | model | StrOutputParser())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2993de",
   "metadata": {},
   "source": [
    "中間値を出力する方法として、ほかに`astream_events`を使う方法もある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94258b7c-13f8-4e6e-884d-3f2003569812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91e23fc3-5a14-473f-828e-bd373103ba0a",
   "metadata": {},
   "source": [
    "# RunnablePassthroughのassingメソッドについて\n",
    "\n",
    "以下では、LangChain Expression Language（LCEL）における `assign` メソッドの役割と使い方、そしてコード内で渡される引数がどのように作用するのかを解説します。\n",
    "\n",
    "## 概要\n",
    "\n",
    "`RunnablePassthrough.assign()` は、チェーンの途中で「入力状態の辞書」をそのまま引き継ぎつつ、新たなキーと値（ランナブル）を追加し、次のステップへの入力として渡すためのヘルパーです。これにより、複数の並行処理や段階的なデータ追加が容易になります。たとえば、Retriever で取り出した文脈（`context`）やプロンプト結果をまとめて次に渡す際に多用されます。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. `assign` の役割と動作原理\n",
    "\n",
    "* **元の状態を保持**しつつ、新しいフィールドを「付け足す」\n",
    "* 返り値は常に「更新後の全辞書」\n",
    "* 内部的には、入力の辞書オブジェクトをクローンし、新たに指定したキーでランナブル（関数やチェーン）を実行した結果を追加します。\n",
    "* LCEL パイプでは、複数の情報ストリームをまとめるときに特に便利です。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. `assign` メソッドのシグネチャ\n",
    "\n",
    "```python\n",
    "RunnablePassthrough.assign(**kwargs)\n",
    "```\n",
    "\n",
    "* `**kwargs` に渡すのは、\n",
    "\n",
    "  * **キー名**：出力の辞書に追加されるフィールド名\n",
    "  * **値**：`Runnable`（たとえば、`prompt | model | parser` など）もしくは関数オブジェクト\n",
    "\n",
    "たとえば：\n",
    "\n",
    "```python\n",
    "RunnablePassthrough.assign(\n",
    "  answer=prompt | model | StrOutputParser()\n",
    ")\n",
    "```\n",
    "\n",
    "これは、\n",
    "\n",
    "1. 入力辞書をそのまま受け取り\n",
    "2. `prompt | model | StrOutputParser()` を実行\n",
    "3. その結果を `\"answer\"` キーにセット\n",
    "4. 更新済み辞書を返す\n",
    "   という動作を一行で実現します。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. コード全体のフロー解説\n",
    "\n",
    "```python\n",
    "import pprint\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"以下文脈を踏まえて質問に答えてください。　文脈：'{context}'　質問：{question}\"\n",
    ")\n",
    "model = ChatOpenAI(...)\n",
    "retriever = TavilySearchAPIRetriever(k=3)\n",
    "\n",
    "chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": retriever,\n",
    "} | RunnablePassthrough.assign(\n",
    "    answer=prompt | model | StrOutputParser()\n",
    ")\n",
    "```\n",
    "\n",
    "1. **Parallel 要素の定義**\n",
    "\n",
    "   ```python\n",
    "   {\n",
    "     \"question\": RunnablePassthrough(),\n",
    "     \"context\": retriever,\n",
    "   }\n",
    "   ```\n",
    "\n",
    "   * ここで、入力辞書から `\"question\"` キーはそのまま・`\"context\"` キーは Retriever 結果として並行取得される状態を作成します。\n",
    "\n",
    "2. **`assign(answer=…)` の適用**\n",
    "\n",
    "   * `RunnableParallel` の出力である辞書（例：`{\"question\": \"...\", \"context\": [...]}`）を受け取り\n",
    "   * `prompt | model | StrOutputParser()` をその辞書をもとに実行\n",
    "   * 生成された文字列を `\"answer\"` キーに追加\n",
    "   * 最終的に `{ \"question\": \"...\", \"context\": [...], \"answer\": \"モデル出力\" }` を返します。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. LCEL におけるデータ受け渡しパターン\n",
    "\n",
    "* **`RunnableParallel`**：複数のランナブルを同じ入力に並行適用し、キーごとの出力をまとめる。\n",
    "* **`RunnablePassthrough()`**：特定のキーを元のまま次ステップに通過させる“パススルー”用。\n",
    "* **`assign`**：上記でまとめた辞書に対し、追加のランナブル実行結果を付与する。\n",
    "\n",
    "これらを組み合わせることで、以下のような典型的フローを実装できます：\n",
    "\n",
    "```python\n",
    "# 1) Retriever で複数ソースから context_a, context_b を取得\n",
    "retrieval = RunnableParallel(\n",
    "    context_a=retriever_a, context_b=retriever_b,\n",
    "    question=RunnablePassthrough()\n",
    ")\n",
    "\n",
    "# 2) assign で answer キーを追加\n",
    "chain = retrieval | RunnablePassthrough.assign(\n",
    "    answer=prompt | model | parser\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. まとめ\n",
    "\n",
    "* `assign` は **“元の辞書を保持しつつ新しいフィールドを付与する”** LCEL の基本プリミティブです。\n",
    "* キー名とランナブル（関数・チェーン）を渡すだけで、その実行結果を辞書にマージし、後続ステップへ繋げます。\n",
    "* `RunnableParallel`＋`RunnablePassthrough.assign` を組み合わせることで、マルチソースからの文脈取得 → プロンプト生成 → モデル呼び出し → 結果格納…といった複雑なフローも簡潔に記述できます。\n",
    "\n",
    "---\n",
    "\n",
    "#### 参考\n",
    "\n",
    "1. How to add values to a chain’s state (assign の基礎) ([python.langchain.com][1])\n",
    "2. RunnablePassthrough API リファレンス ([python.langchain.com][2])\n",
    "3. データを次ステップへそのまま通す方法 ([python.langchain.com][3])\n",
    "4. LCEL フロー例（Pinecone 解説） ([pinecone.io][4])\n",
    "5. ソースドキュメントを返す例（StackOverflow） ([stackoverflow.com][5])\n",
    "6. LCEL チートシート ([python.langchain.com][6])\n",
    "7. GitHub discussion: assign の活用例 ([github.com][7])\n",
    "8. LCEL 変数のアクセス（StackOverflow） ([stackoverflow.com][8])\n",
    "9. JavaScript版 assign 解説 ([js.langchain.com][9])\n",
    "10. assign でパススルー＋追加フィールド ([github.com][10])\n",
    "\n",
    "以上を参考に、`assign` を使いこなし、LCEL でのより複雑なチェーン構築にお役立てください。\n",
    "\n",
    "[1]: https://python.langchain.com/docs/how_to/assign/?utm_source=chatgpt.com \"How to add values to a chain's state - Python LangChain\"\n",
    "[2]: https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html?utm_source=chatgpt.com \"RunnablePassthrough — LangChain documentation\"\n",
    "[3]: https://python.langchain.com/docs/how_to/passthrough/?utm_source=chatgpt.com \"How to pass through arguments from one step to the next\"\n",
    "[4]: https://www.pinecone.io/learn/series/langchain/langchain-expression-language/?utm_source=chatgpt.com \"LangChain Expression Language Explained - Pinecone\"\n",
    "[5]: https://stackoverflow.com/questions/77759685/how-to-return-source-documents-when-using-langchain-expression-language-lcel?utm_source=chatgpt.com \"How to return source documents when using LangChain Expression ...\"\n",
    "[6]: https://python.langchain.com/docs/how_to/lcel_cheatsheet/?utm_source=chatgpt.com \"LangChain Expression Language Cheatsheet\"\n",
    "[7]: https://github.com/langchain-ai/langchain/discussions/23532?utm_source=chatgpt.com \"RunnablePassthrough.assign(context=(lambda x: format_docs(x ...\"\n",
    "[8]: https://stackoverflow.com/questions/78379953/accessing-langchain-lcel-variables-from-prior-steps-in-the-chain?utm_source=chatgpt.com \"Accessing LangChain LCEL variables from prior steps in the chain\"\n",
    "[9]: https://js.langchain.com/docs/how_to/assign/?utm_source=chatgpt.com \"How to add values to a chain's state - LangChain.js\"\n",
    "[10]: https://github.com/langchain-ai/langchain/discussions/18311?utm_source=chatgpt.com \"How to pass arguments to a function in LCEL #18311 - GitHub\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f666b3-f238-425b-8e44-9d7579e4e0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa8d7e07-bc01-4f4f-bb67-88f74f19719d",
   "metadata": {},
   "source": [
    "# chain.invoke()に渡す引数の型について\n",
    "\n",
    "`chain.invoke()` が受け取る引数は常に **ひとつだけ** です。これは LCEL（LangChain Expression Language）の設計上の制約で、複数の引数を個別に渡すことはできません。では、その「ひとつ」の引数に何を渡せばよいか、以下のポイントで整理します。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 何を渡すかはチェーンの最初のステップ次第\n",
    "\n",
    "* **RunnableParallel（もしくは辞書リテラル）** でチェーンを構築した場合\n",
    "\n",
    "  ```python\n",
    "  chain = RunnableParallel({\n",
    "      \"question\": RunnablePassthrough(),\n",
    "      \"context\": retriever,\n",
    "  }) | RunnablePassthrough.assign(\n",
    "      answer = prompt | model\n",
    "  )\n",
    "  ```\n",
    "\n",
    "  のように定義すると、`RunnableParallel` は全てのサブランナブルに同じ入力を同時に投げます。\n",
    "\n",
    "  * `RunnablePassthrough()` は入力をそのまま返すランナブルなので、`\"question\"` キーにはそのまま渡した値が入ります。\n",
    "  * `retriever` は文字列クエリを受け取る設計のランナブルなので、同じく文字列を受け取ります。\n",
    "\n",
    "  その結果、**文字列（`str`）をひとつ**渡せばよい、ということになります。その文字列が `\"question\"` に入り、同じ文字列が `retriever` のクエリとして使われます。\n",
    "\n",
    "  ```python\n",
    "  result = chain.invoke(\"LangChainの概要を教えてください。\")\n",
    "  ```\n",
    "\n",
    "  ([python.langchain.com][1])\n",
    "\n",
    "---\n",
    "\n",
    "## 2. dict を渡すパターン\n",
    "\n",
    "もし最初のステップで、サブランナブルのうちどれかが **辞書型の入力**を期待している場合（たとえば複数の異なるキーを別々に扱いたいときなど）、引数には **キー名を合わせた辞書** を渡します。\n",
    "\n",
    "```python\n",
    "chain = RunnableParallel({\n",
    "    \"query\": lambda x: x[\"text\"],  # dict の \"text\" キーを参照する\n",
    "    \"meta\": SomeRunnable(),\n",
    "})\n",
    "```\n",
    "\n",
    "このようなチェーンなら、\n",
    "\n",
    "```python\n",
    "# {\"text\": \"...\"} という dict を渡す\n",
    "chain.invoke({\"text\": \"質問内容をここに\", \"other\": \"...\"})\n",
    "```\n",
    "\n",
    "のように、辞書を渡す必要があります。\n",
    "([python.langchain.com][2])\n",
    "\n",
    "---\n",
    "\n",
    "## 3. チェーンが期待する入力キーを調べる方法\n",
    "\n",
    "実際にどんな引数を入れればいいか迷ったら、**`.input_keys`** プロパティで確認できます。\n",
    "\n",
    "```python\n",
    "print(chain.input_keys)\n",
    "# → ['question']  のように出れば、文字列を入れたときに \"question\" として扱われます。\n",
    "```\n",
    "\n",
    "* `['question']` なら文字列を渡すと `\"question\"` キーに自動マッピングされる\n",
    "* `['text', 'other']` など複数キーなら、それらを含む辞書を渡す\n",
    "\n",
    "---\n",
    "\n",
    "## 4. まとめ\n",
    "\n",
    "1. **文字列を渡す**\n",
    "\n",
    "   * チェーンの最初が `RunnablePassthrough()` と文字列クエリ対応の Retriever であれば、`chain.invoke(\"...\")` で OK。\n",
    "2. **辞書を渡す**\n",
    "\n",
    "   * 最初のランナブルが辞書型入力を期待している場合は、キー名を揃えた辞書を `chain.invoke({...})` で渡す。\n",
    "3. **確認は `.input_keys`**\n",
    "\n",
    "   * `chain.input_keys` で何を要求されているかを見るのが最も確実です。\n",
    "\n",
    "このように、**チェーンの最初にあるランナブルたちが受け取る型**を揃えれば、`chain.invoke()` の引数は文字列でも辞書でも問題なく動作します。\n",
    "\n",
    "[1]: https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html?utm_source=chatgpt.com \"RunnableParallel — LangChain documentation\"\n",
    "[2]: https://python.langchain.com/docs/concepts/lcel/?utm_source=chatgpt.com \"LangChain Expression Language (LCEL)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44d3b73-6e81-47cb-8951-0daed7b22f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
