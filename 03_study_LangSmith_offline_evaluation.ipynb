{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70949756",
   "metadata": {},
   "source": [
    "## LangSmith と Ragas を使ったオフライン評価\n",
    "\n",
    "- オフライン評価では、LangSmith クライアント提供の「evaluate」関数を使う。\n",
    "- 引数に推論の関数、Dataset の名前、Evaluator（評価器）を指定して evaluate 関数を実行すると、LangSmith の画面上で評価結果を確認できる。\n",
    "\n",
    "### 利用可能な Evaluator（評価器）\n",
    "\n",
    "- LangChain が提供する Evaluator では、評価用のプロンプトを使った LLM による評価、埋め込みベクトルの類似度やレーベンシュタイン距離による評価といった機能が提供されている。\n",
    "- 独自定義関数も使用可能\n",
    "\n",
    "#### ここでは、Ragas の評価メトリックを以下で使用\n",
    "\n",
    "### Ragas の評価メトリクス\n",
    "\n",
    "- RAG の評価軸：検索、生成、検索+生成\n",
    "- Ragas では、これらに対して、評価メトリクスを提供\n",
    "\n",
    "#### 「検索」の評価メトリクス：\n",
    "\n",
    "- Context recall, Context precision, Context entity recall\n",
    "\n",
    "#### 「生成」の評価メトリクス：\n",
    "\n",
    "- Faithfulness, Answer relevancy\n",
    "\n",
    "#### 「検索+生成」の評価メトリクス：\n",
    "\n",
    "- Answer similarity, Answer correctness\n",
    "\n",
    "#### 評価詳細：\n",
    "\n",
    "##### -検索-\n",
    "\n",
    "- Context precision - 質問と期待する回答を踏まえて、実際の検索結果のうち有用だと LLM で推論される割合（LLM 使用）\n",
    "- Context recall - 期待する回答をいくつかの文章に分割したうち、実際の検索結果で説明できる割合（LLM 使用）\n",
    "- Context entity recall - 期待する回答に含まれるエンティティ（物事）のうち、実際の検索結果で説明できる割合（LLM 使用）\n",
    "\n",
    "##### -生成-\n",
    "\n",
    "- Answer relevancy - 実際の回答が質問にどれだけ関連するか（実際の回答から LLM で推論した質問と、もとの質問の、埋め込みベクトルのコサイン類似度の平均値）（LLM 使用/Embedding 使用）\n",
    "- Faithfulness - 実際の回答に含まれる主張のうち、実際の検索結果と一貫している割合（LLM 使用）\n",
    "\n",
    "##### -検索+生成-\n",
    "\n",
    "- Answer similarity - 実際の回答と期待する回答の、埋め込みベクトルのコサイン類似度（Embedding 使用）\n",
    "- Answer correctness - 実際の回答と期待する回答の、事実的類似性と意味的類似性(Answer similarity)の加重平均（LLM 使用/Embedding 使用）\n",
    "\n",
    "上記 Ragas の各評価メトリクスは、「期待する検索結果」を一切使わずに、実装されている。  \n",
    "「期待する検索結果」がデータセットに含まれる場合は、以下の評価指標を使える\n",
    "\n",
    "- Recall - 期待する検索結果のうち、実際の検索結果に含まれる割合\n",
    "- Precision - 実際の検索結果のうち、期待する検索結果の割合\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec35c75",
   "metadata": {},
   "source": [
    "### カスタム Evaluator 実装\n",
    "\n",
    "- カスタム Evaluator は、実際の実行結果（Run）と評価データ（Example）を引数として評価スコアを dict で返す関数として実装できる\n",
    "- Ragas の評価メトリクスを使うときは、使用する LLM や Embedding モデルを設定する必要がある\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eabf96dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f42a9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langsmith.schemas import Example, Run\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics.base import Metric, MetricWithEmbeddings, MetricWithLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42861661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagasMetricEvaluator:\n",
    "    def __init__(self, metric: Metric, llm: BaseChatModel, embeddings: Embeddings):\n",
    "        self.metric = metric\n",
    "        if isinstance(self.metric, MetricWithLLM):\n",
    "            self.metric.llm = LangchainLLMWrapper(llm)\n",
    "        if isinstance(self.metric, MetricWithEmbeddings):\n",
    "            self.metric.embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "    def evaluate(self, run: Run, example: Example) -> dict[str, Any]:\n",
    "        # ① 文脈文字列リストを 'retrieved_contexts' から作成\n",
    "        contexts = [doc.page_content for doc in run.outputs[\"retrieved_contexts\"]]\n",
    "\n",
    "        # ② 必要なキー名に合わせて辞書を構築\n",
    "        row = {\n",
    "            # 'question' ではなく 'user_input'\n",
    "            \"user_input\": example.inputs.get(\"user_input\"),\n",
    "            # 'answer' ではなく 'response'\n",
    "            \"response\": run.outputs[\"response\"],\n",
    "            # そのまま 'retrieved_contexts'\n",
    "            \"retrieved_contexts\": contexts,\n",
    "            # 'ground_truth' ではなく 'reference'\n",
    "            \"reference\": example.outputs.get(\"reference\"),\n",
    "        }\n",
    "\n",
    "        # ③ スコア算出\n",
    "        score = self.metric.score(row)\n",
    "        return {\"key\": self.metric.name, \"score\": score}\n",
    "\n",
    "\n",
    "# class RagasMetricEvaluator:\n",
    "#     def __init__(self, metric: Metric, llm: BaseChatModel, embeddings: Embeddings):\n",
    "#         self.metric = metric\n",
    "\n",
    "#         # LLMとEmbeddingをMetricに設定\n",
    "#         if isinstance(self.metric, MetricWithLLM):\n",
    "#             self.metric.llm = LangchainLLMWrapper(llm)\n",
    "#         if isinstance(self.metric, MetricWithEmbeddings):\n",
    "#             self.metric.embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "#     def evaluate(self, run: Run, example: Example) -> dict[str, Any]:\n",
    "#         context_strs = [doc.page_content for doc in run.outputs[\"contexts\"]]\n",
    "\n",
    "#         # Ragasの評価メトリクスのscoreメソッドでスコアを算出\n",
    "#         score = self.metric.score(\n",
    "#             {\n",
    "#                 \"question\": example.inputs[\"question\"],  # 質問\n",
    "#                 \"answer\": run.outputs[\"answer\"],  # 実際の回答\n",
    "#                 \"contexts\": context_strs,  # 実際の検索結果\n",
    "#                 \"ground_truth\": example.outputs[\"ground_truth\"],  # 期待する回答\n",
    "#             },\n",
    "#         )\n",
    "#         return {\"key\": self.metric.name, \"score\": score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1369b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.metrics import answer_relevancy, context_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1128489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metircs = [context_precision, answer_relevancy]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "evaluators = [\n",
    "    RagasMetricEvaluator(metric, llm, embeddings).evaluate for metric in metircs\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957af8a",
   "metadata": {},
   "source": [
    "### 推論関数の実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8667039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "111a40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈：'''\n",
    "{context}\n",
    "'''\n",
    "\n",
    "質問：{question}\n",
    "\"\"\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": retriever,\n",
    "    }\n",
    ").assign(answer=prompt | model | StrOutputParser())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0450a8",
   "metadata": {},
   "source": [
    "LangSmith での評価に使う推論関数は、データセットに保存した形式の dict を受け取り、実際の実行結果(Run)を dict して返す関数として実装する。  \n",
    "データセットの入力から「question」を取り出して、推論（RAG などの処理）を行い、実際の検索結果や回答を返す関数は以下のようになる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2543818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Ragasの評価用に実行する推論関数。\n",
    "    データセットの入力から「user_input」を取り出して推論し、\n",
    "    必要なキー名で返却する。\n",
    "    \"\"\"\n",
    "    # “question” ではなく “user_input” を使う\n",
    "    user_input = inputs[\"user_input\"]\n",
    "\n",
    "    # RAG チェーンを実行\n",
    "    output = chain.invoke(user_input)\n",
    "\n",
    "    return {\n",
    "        # “answer” ではなく “response”\n",
    "        \"response\": output[\"answer\"],\n",
    "        # “contexts” ではなく “retrieved_contexts”\n",
    "        \"retrieved_contexts\": output[\"context\"],\n",
    "        # もしデータセットに “reference” があるならそれも返す\n",
    "        \"reference\": inputs.get(\"reference\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# def predict(inputs: dict[str, Any]) -> dict[str, Any]:\n",
    "#     \"\"\"\n",
    "#     Ragasの評価用に実行する推論関数。\n",
    "#     データセットの入力から「question」を取り出して、推論（RAGなどの処理）を行い、実際の検索結果や回答を返す。\n",
    "#     \"\"\"\n",
    "#     question = inputs[\"question\"]\n",
    "#     output = chain.invoke(question)\n",
    "\n",
    "#     return {\n",
    "#         \"answer\": output[\"answer\"],\n",
    "#         \"contexts\": output[\"context\"],\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4195b34e",
   "metadata": {},
   "source": [
    "### オフライン評価の実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fbdcb9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'tart-theory-87' at:\n",
      "https://smith.langchain.com/o/5cb6609d-ce31-51cc-962f-6052d0aff4cc/datasets/f453892f-9c24-486b-9209-e651798b8cbf/compare?selectedSessions=b067fed9-1391-44f1-918b-46933270f057\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13256a4eab4542188f8a6f9b4af6c5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yoshi\\AppData\\Local\\Temp\\ipykernel_15116\\1225277247.py:29: DeprecationWarning: The function score was deprecated in 0.2, and will be removed in the 0.3 release. Use single_turn_ascore instead.\n",
      "  score = self.metric.score(row)\n",
      "C:\\Users\\yoshi\\AppData\\Local\\Temp\\ipykernel_15116\\1225277247.py:29: DeprecationWarning: The function score was deprecated in 0.2, and will be removed in the 0.3 release. Use single_turn_ascore instead.\n",
      "  score = self.metric.score(row)\n",
      "C:\\Users\\yoshi\\AppData\\Local\\Temp\\ipykernel_15116\\1225277247.py:29: DeprecationWarning: The function score was deprecated in 0.2, and will be removed in the 0.3 release. Use single_turn_ascore instead.\n",
      "  score = self.metric.score(row)\n",
      "C:\\Users\\yoshi\\AppData\\Local\\Temp\\ipykernel_15116\\1225277247.py:29: DeprecationWarning: The function score was deprecated in 0.2, and will be removed in the 0.3 release. Use single_turn_ascore instead.\n",
      "  score = self.metric.score(row)\n",
      "C:\\Users\\yoshi\\AppData\\Local\\Temp\\ipykernel_15116\\1225277247.py:29: DeprecationWarning: The function score was deprecated in 0.2, and will be removed in the 0.3 release. Use single_turn_ascore instead.\n",
      "  score = self.metric.score(row)\n",
      "C:\\Users\\yoshi\\AppData\\Local\\Temp\\ipykernel_15116\\1225277247.py:29: DeprecationWarning: The function score was deprecated in 0.2, and will be removed in the 0.3 release. Use single_turn_ascore instead.\n",
      "  score = self.metric.score(row)\n",
      "C:\\Users\\yoshi\\AppData\\Local\\Temp\\ipykernel_15116\\1225277247.py:29: DeprecationWarning: The function score was deprecated in 0.2, and will be removed in the 0.3 release. Use single_turn_ascore instead.\n",
      "  score = self.metric.score(row)\n",
      "C:\\Users\\yoshi\\AppData\\Local\\Temp\\ipykernel_15116\\1225277247.py:29: DeprecationWarning: The function score was deprecated in 0.2, and will be removed in the 0.3 release. Use single_turn_ascore instead.\n",
      "  score = self.metric.score(row)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.user_input</th>\n",
       "      <th>outputs.response</th>\n",
       "      <th>outputs.retrieved_contexts</th>\n",
       "      <th>outputs.reference</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.response</th>\n",
       "      <th>reference.reference</th>\n",
       "      <th>reference.retrieved_contexts</th>\n",
       "      <th>feedback.context_precision</th>\n",
       "      <th>feedback.answer_relevancy</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does LLM observability boost security in L...</td>\n",
       "      <td>LLM observability enhances security in Layerup...</td>\n",
       "      <td>[page_content='# Layerup Security\\n\\nThe [Laye...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>[# PromptLayer\\n\\n&gt;[PromptLayer](https://docs....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.994580</td>\n",
       "      <td>2.440248</td>\n",
       "      <td>569b4ed4-bc0f-4f20-add7-c425959f379e</td>\n",
       "      <td>8e58a1db-1e9a-45ec-b468-bf57c1189571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does MLflow simplify LLM provider interact...</td>\n",
       "      <td>MLflow simplifies interactions with various la...</td>\n",
       "      <td>[page_content='# MLflow AI Gateway for LLMs\\n\\...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MLflow simplifies LLM provider interactions by...</td>\n",
       "      <td>MLflow simplifies LLM provider interactions by...</td>\n",
       "      <td>[# MLflow Deployments for LLMs\\n\\n&gt;[The MLflow...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.828796</td>\n",
       "      <td>2.527124</td>\n",
       "      <td>3111f78c-231b-4c60-ba38-2b33cdf8b234</td>\n",
       "      <td>e40d0772-67f3-448f-99bb-f521cf619231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the purpose of creating an API token w...</td>\n",
       "      <td>The purpose of creating an API token when work...</td>\n",
       "      <td>[page_content='# PromptLayer\\n\\n&gt;[PromptLayer]...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>The purpose of creating an API token when work...</td>\n",
       "      <td>The purpose of creating an API token when work...</td>\n",
       "      <td>[# PromptLayer\\n\\n&gt;[PromptLayer](https://docs....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998847</td>\n",
       "      <td>1.627504</td>\n",
       "      <td>0e656d1f-2674-4ec9-914a-83b7e82fdc20</td>\n",
       "      <td>7637ceb0-c322-4541-833e-af8a1307e747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What features are provided by the instant mess...</td>\n",
       "      <td>Telegram provides several features, including:...</td>\n",
       "      <td>[page_content='# Telegram\\n\\n&gt;[Telegram Messen...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Telegram provides features such as end-to-end ...</td>\n",
       "      <td>Telegram provides features such as end-to-end ...</td>\n",
       "      <td>[# Telegram\\n\\n&gt;[Telegram Messenger](https://w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.903376</td>\n",
       "      <td>3.731595</td>\n",
       "      <td>c509ef6b-86e1-4ad8-9272-bd71fb2a00f9</td>\n",
       "      <td>ab0adfd7-254e-41b7-9703-2c871a671096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults tart-theory-87>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "evaluate(\n",
    "    predict,\n",
    "    data=\"test-dataset-2\",\n",
    "    evaluators=evaluators,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c34b3",
   "metadata": {},
   "source": [
    "上記実行後、LangSmith に評価結果が保存される。  \n",
    "LangSmith では、評価の１回１回を「Experiment」と呼ぶ。  \n",
    "detaset の、「Experiment」タブから結果を見ることができる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9504564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
